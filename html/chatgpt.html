<!DOCTYPE html>
<html lang="fr">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Comment fonctionne ChatGPT</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body class="chatgpt-page">
    <header class="main-header">
        <div class="logo">
            <span>OpenAI</span>
            <img src="images/openia.png" alt="Logo OpenAI" class="logo-img">
        </div>
        <nav class="main-nav">
            <ul>
                <li><a href="index.html">Accueil</a></li>
                <li><a href="chatgpt.html">Fonctionnement</a></li>
                <li><a href="about.html">À propos</a></li>
            </ul>
        </nav>
    </header>

    <main class="content">
        <h1>Methods</h1>
        <p>
            Nous avons formé ce modèle à l'aide de l'apprentissage par renforcement à partir de la rétroaction humaine (RLHF), en utilisant les mêmes méthodes que
            InstructGPT⁠, mais avec de légères différences dans la configuration de la collecte de données. Nous avons formé un premier modèle en utilisant
            mise au point supervisée : des formateurs humains en IA ont fourni des conversations dans lesquelles ils ont joué des deux côtés : l'utilisateur
            et un assistant IA. Nous avons donné aux formateurs accès à des suggestions rédigées sous forme de modèles pour les aider à composer leur
            réponses. Nous avons mélangé ce nouvel ensemble de données de dialogue avec l'ensemble de données InstructGPT, que nous avons transformé en un
            format de dialogue.

            <br><br>
            Pour créer un modèle de récompense pour l'apprentissage par renforcement, nous devions collecter des données de comparaison, consistant
            de deux ou plusieurs réponses modèles classées par qualité. Pour collecter ces données, nous avons pris des conversations que l'IA
            les formateurs ont eu avec le chatbot. Nous avons sélectionné au hasard un modèle de message écrit, échantillonné plusieurs alternatives
            réussites, et les formateurs en IA les ont classés. En utilisant ces modèles de récompense, nous pouvons affiner le modèle en utilisant
            Optimisation de la politique proximale⁠. Nous avons effectué plusieurs itérations de ce processus.</h1>
        </p>
        <img src="images/ex1.png" alt="" class="gougou">
        <p>ChatGPT is fine-tuned from a model in the GPT-3.5 series, which finished training in early 2022. You can
            learn more about the 3.5 series here⁠(opens in a new window). ChatGPT and GPT-3.5 were trained on an Azure
            AI supercomputing infrastructure.
        </p>

        <h1>Limitations</h1>
        <p> ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers. Fixing this issue is
            challenging, as: (1) during RL training, there’s currently no source of truth; (2) training the model to be
            more cautious causes it to decline questions that it can answer correctly; and (3) supervised training
            misleads the model because the ideal answer depends on what the model knows⁠(opens in a new window), rather
            than what the human demonstrator knows.

            ChatGPT is sensitive to tweaks to the input phrasing or attempting the same prompt multiple times. For
            example, given one phrasing of a question, the model can claim to not know the answer, but given a slight
            rephrase, can answer correctly.

            The model is often excessively verbose and overuses certain phrases, such as restating that it’s a language
            model trained by OpenAI. These issues arise from biases in the training data (trainers prefer longer answers
            that look more comprehensive) and well-known over-optimization issues.1, 2

            Ideally, the model would ask clarifying questions when the user provided an ambiguous query. Instead, our
            current models usually guess what the user intended.

            While we’ve made efforts to make the model refuse inappropriate requests, it will sometimes respond to
            harmful instructions or exhibit biased behavior. We’re using the Moderation API⁠ to warn or block certain
            types of unsafe content, but we expect it to have some false negatives and positives for now. We’re eager to
            collect user feedback to aid our ongoing work to improve this system.</p>
        <h1>Iterative deployment</h1>

        <p>Today’s research release of ChatGPT is the latest step in OpenAI’s iterative deployment⁠ of increasingly safe
            and useful AI systems. Many lessons from deployment of earlier models like GPT-3 and Codex have informed the
            safety mitigations in place for this release, including substantial reductions in harmful and untruthful
            outputs achieved by the use of reinforcement learning from human feedback (RLHF).
        </p>















    </main>




    <footer class="footer">
        <p>&copy; 2024 OpenAI. Tous droits réservés.</p>
    </footer>
</body>

</html>




